{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory in the world of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a random variable is the average level of uncertainty associated with the variables potential state\\\n",
    "The measure of the expected amount of information to describe the state of the variable condisering the distribution of probabilities across all potential states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import log2\n",
    "\n",
    "def entropy(probabilities: List[int])->int:\n",
    "    H = -sum(p * log2(p) for p in probabilities if p > 0)  \n",
    "    return H\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "probabilities = [0.25, 0.25, 0.25,0.25]\n",
    "\n",
    "try:\n",
    "    sum(probabilities) == 1\n",
    "except:\n",
    "    print(\"Error: The probabilities are not valid\")\n",
    "    \n",
    "print(entropy(probabilities=probabilities))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the measure of the average amount of information contained in a message\\\n",
    "It quantifies the unpredictability of info content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the unique characters in the message:\n",
      "(' ', 1)\n",
      "('H', 1)\n",
      "('d', 1)\n",
      "('e', 1)\n",
      "('l', 3)\n",
      "('o', 2)\n",
      "('r', 1)\n",
      "('w', 1)\n",
      "Shannon entropy of 'Hello world': 2.85 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shannon_entropy(data):\n",
    "    chars, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "    # Count of the unique characters in the message\n",
    "    char_counts = list(zip(chars, counts))\n",
    "    print(\"Count of the unique characters in the message:\")\n",
    "    for char, count in char_counts:\n",
    "        print(f\"('{char}', {count})\")\n",
    "\n",
    "    # Compute Shannon entropy\n",
    "    probabilities = counts / len(data)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "# Example: Calculate Shannon entropy for a text message\n",
    "message = \"Hello world\"\n",
    "print(f\"Shannon entropy of '{message}': {shannon_entropy(list(message)):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since entropy is the measure of uncertainty and the objective of ML is to minimize the uncertainty the two are linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the measure of the reduction in Entropy achieved by splitting a dataset according to a particular feature (this is used in tree algorithms to select the features)\\\n",
    "This is the amount of information a feature can provide about a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\\\n",
    "We have a dataset with cancerous (C) and non cancerous cells (NC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Samples  Mutation 1  Mutation 2  Mutation 3  Mutation 4\n",
      "0      C1           1           1           1           0\n",
      "1      C2           1           1           0           1\n",
      "2      C3           1           0           1           1\n",
      "3      C4           0           1           1           0\n",
      "4     NC1           0           0           0           0\n",
      "5     NC2           0           1           0           0\n",
      "6     NC3           1           1           0           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = {\n",
    "    'Samples': ['C1', 'C2', 'C3', 'C4', 'NC1', 'NC2', 'NC3'],\n",
    "    'Mutation 1': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'Mutation 2': [1, 1, 0, 1, 0, 1, 1],\n",
    "    'Mutation 3': [1, 0, 1, 1, 0, 0, 0],\n",
    "    'Mutation 4': [0, 1, 1, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, index=None)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a very simple decision tree with 1 parent node which is highly impute with all the features and 2 pure child nodes one with just the cancerous cells and the other one all the non cancerous cells\\\n",
    "Then we wanna know how to split the data in order to classify the future nodes the best we can (which means than the node childs 1 and 2 must be as pure a possible)\n",
    "\n",
    " **Parent Node:** The parent node is represented with its high impurity \n",
    "* **Child Nodes:** The child nodes are indented using bullet points and indicate the outcome (True or False) of the parent node's decision.\n",
    "* **Class and Probability:** Each child node includes the predicted class (e.g., Class A) and its associated probability (e.g., p1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# We can calculate the Entropy for the feature Mutation 1\n",
    "prob_zeros = (df['Mutation 1'] == 0).sum()/df['Mutation 1'].shape[0]\n",
    "prob_ones = (df['Mutation 1'] == 1).sum()/df['Mutation 1'].shape[0]\n",
    "print(prob_zeros)\n",
    "print(prob_ones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
